{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0cf3427",
   "metadata": {},
   "source": [
    "# Using BERT + BiLSTM as Sentiment Analyzer\n",
    "## Unified preprocessed:\n",
    "1. 字母小寫\n",
    "2. 刪除網址\n",
    "3. 移除標點符號（所有標點符號都要刪除）\n",
    "4. 移除非英文字母\n",
    "5. 計算類別權重（class_weight）\n",
    "6. 移除停用詞（stop_words）\n",
    "7. 不替換用戶名（replace_username：False）\n",
    "8. 不替換 COVID 相關詞彙（replace_covid：False）\n",
    "\n",
    "### 2025/05/29 01:49 by sky\n",
    "## gradient search for following hyperparameter：\n",
    "### 階段一\n",
    "    batch sizes = [64, 128, 256]\n",
    "    學習率=[0.01,0.001, 0.0001]\n",
    "### 階段二\n",
    "    hidden_dim=[128, 256, 512]\n",
    "    num_layers=[2, 3, 4]\n",
    "    dropout=[0.1, 0.2, 0.3]\n",
    "### 階段三\n",
    "    max lengths = [30, 50]\n",
    "### 階段四\n",
    "    pooling methods = ['hidden state', 'max pooling', 'mean pooling']\n",
    "    activation function：[None, ReLU, tanh]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae9737e",
   "metadata": {},
   "source": [
    "## 步驟1：載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf28f74-fe20-4d48-bbec-2794f7c59673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a89157e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\__init__.py:48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     ArrowDtype,\n\u001b[0;32m     51\u001b[0m     Int8Dtype,\n\u001b[0;32m     52\u001b[0m     Int16Dtype,\n\u001b[0;32m     53\u001b[0m     Int32Dtype,\n\u001b[0;32m     54\u001b[0m     Int64Dtype,\n\u001b[0;32m     55\u001b[0m     UInt8Dtype,\n\u001b[0;32m     56\u001b[0m     UInt16Dtype,\n\u001b[0;32m     57\u001b[0m     UInt32Dtype,\n\u001b[0;32m     58\u001b[0m     UInt64Dtype,\n\u001b[0;32m     59\u001b[0m     Float32Dtype,\n\u001b[0;32m     60\u001b[0m     Float64Dtype,\n\u001b[0;32m     61\u001b[0m     CategoricalDtype,\n\u001b[0;32m     62\u001b[0m     PeriodDtype,\n\u001b[0;32m     63\u001b[0m     IntervalDtype,\n\u001b[0;32m     64\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     65\u001b[0m     StringDtype,\n\u001b[0;32m     66\u001b[0m     BooleanDtype,\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     NA,\n\u001b[0;32m     69\u001b[0m     isna,\n\u001b[0;32m     70\u001b[0m     isnull,\n\u001b[0;32m     71\u001b[0m     notna,\n\u001b[0;32m     72\u001b[0m     notnull,\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     Index,\n\u001b[0;32m     75\u001b[0m     CategoricalIndex,\n\u001b[0;32m     76\u001b[0m     RangeIndex,\n\u001b[0;32m     77\u001b[0m     MultiIndex,\n\u001b[0;32m     78\u001b[0m     IntervalIndex,\n\u001b[0;32m     79\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     80\u001b[0m     DatetimeIndex,\n\u001b[0;32m     81\u001b[0m     PeriodIndex,\n\u001b[0;32m     82\u001b[0m     IndexSlice,\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     NaT,\n\u001b[0;32m     85\u001b[0m     Period,\n\u001b[0;32m     86\u001b[0m     period_range,\n\u001b[0;32m     87\u001b[0m     Timedelta,\n\u001b[0;32m     88\u001b[0m     timedelta_range,\n\u001b[0;32m     89\u001b[0m     Timestamp,\n\u001b[0;32m     90\u001b[0m     date_range,\n\u001b[0;32m     91\u001b[0m     bdate_range,\n\u001b[0;32m     92\u001b[0m     Interval,\n\u001b[0;32m     93\u001b[0m     interval_range,\n\u001b[0;32m     94\u001b[0m     DateOffset,\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     to_numeric,\n\u001b[0;32m     97\u001b[0m     to_datetime,\n\u001b[0;32m     98\u001b[0m     to_timedelta,\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     Flags,\n\u001b[0;32m    101\u001b[0m     Grouper,\n\u001b[0;32m    102\u001b[0m     factorize,\n\u001b[0;32m    103\u001b[0m     unique,\n\u001b[0;32m    104\u001b[0m     value_counts,\n\u001b[0;32m    105\u001b[0m     NamedAgg,\n\u001b[0;32m    106\u001b[0m     array,\n\u001b[0;32m    107\u001b[0m     Categorical,\n\u001b[0;32m    108\u001b[0m     set_eng_float_format,\n\u001b[0;32m    109\u001b[0m     Series,\n\u001b[0;32m    110\u001b[0m     DataFrame,\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\api.py:47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstruction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flags\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     48\u001b[0m     Grouper,\n\u001b[0;32m     49\u001b[0m     NamedAgg,\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     52\u001b[0m     CategoricalIndex,\n\u001b[0;32m     53\u001b[0m     DatetimeIndex,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatetimes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     62\u001b[0m     bdate_range,\n\u001b[0;32m     63\u001b[0m     date_range,\n\u001b[0;32m     64\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\groupby\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     DataFrameGroupBy,\n\u001b[0;32m      3\u001b[0m     NamedAgg,\n\u001b[0;32m      4\u001b[0m     SeriesGroupBy,\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GroupBy\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Grouper\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:77\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     71\u001b[0m     GroupByApply,\n\u001b[0;32m     72\u001b[0m     maybe_mangle_lambdas,\n\u001b[0;32m     73\u001b[0m     reconstruct_func,\n\u001b[0;32m     74\u001b[0m     validate_func_kwargs,\n\u001b[0;32m     75\u001b[0m )\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcom\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     80\u001b[0m     GroupBy,\n\u001b[0;32m     81\u001b[0m     GroupByPlot,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m     _transform_template,\n\u001b[0;32m     85\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:182\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseFrameAccessor\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstruction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    177\u001b[0m     ensure_wrapped_if_datetimelike,\n\u001b[0;32m    178\u001b[0m     extract_array,\n\u001b[0;32m    179\u001b[0m     sanitize_array,\n\u001b[0;32m    180\u001b[0m     sanitize_masked_array,\n\u001b[0;32m    181\u001b[0m )\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NDFrame\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_key_length\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    185\u001b[0m     DatetimeIndex,\n\u001b[0;32m    186\u001b[0m     Index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m     ensure_index_from_sequences,\n\u001b[0;32m    191\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:169\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    161\u001b[0m     ArrayManager,\n\u001b[0;32m    162\u001b[0m     BlockManager,\n\u001b[0;32m    163\u001b[0m     SingleArrayManager,\n\u001b[0;32m    164\u001b[0m )\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstruction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    166\u001b[0m     mgr_to_mgr,\n\u001b[0;32m    167\u001b[0m     ndarray_to_mgr,\n\u001b[0;32m    168\u001b[0m )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmethods\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdescribe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m describe_ndframe\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    171\u001b[0m     clean_fill_method,\n\u001b[0;32m    172\u001b[0m     clean_reindex_fill_method,\n\u001b[0;32m    173\u001b[0m     find_valid_index,\n\u001b[0;32m    174\u001b[0m )\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m align_method_FRAME\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\methods\\describe.py:41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtype\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowDtype\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfloating\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Float64Dtype\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m concat\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m format_percentiles\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:982\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:925\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1423\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1395\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1526\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1569\u001b[0m, in \u001b[0;36m_fill_cache\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import re\n",
    "import string\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.font_manager as fm\n",
    "from itertools import product\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b03fcc-bdb7-406e-8ff5-12500e664ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c6174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設置中文字型\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c48c1ed",
   "metadata": {},
   "source": [
    "# 步驟2：設置參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 5  # 五種情感類別\n",
    "EPOCHS = 15  # 訓練輪數\n",
    "PATIENCE = 5  # 早停耐心值\n",
    "CLIP_GRAD_NORM = 1.0  # 梯度裁剪範圍\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba7bf5-650e-40f8-8c67-00d966697c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = {\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 0.0001,\n",
    "    'hidden_dim': 256,\n",
    "    'num_layers': 4,\n",
    "    'dropout': 0.2,\n",
    "    'max_length': 50,\n",
    "    'pooling_method': 'max pooling',  # 修正為 'max pooling'\n",
    "    'activation_function': 'softmax'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a620c8-9c38-410c-b42d-154825c547e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 階段性參數\n",
    "stage_params = {\n",
    "    'stage1': {\n",
    "        'batch_size': [64, 128, 256],\n",
    "        'learning_rate': [0.01, 0.001, 0.0001]\n",
    "    },\n",
    "    'stage2': {\n",
    "        'hidden_dim': [128, 256, 512],\n",
    "        'num_layers': [2, 3, 4],\n",
    "        'dropout': [0.1, 0.2, 0.3]\n",
    "    },\n",
    "    'stage3': {\n",
    "        'max_length': [30, 50]\n",
    "    },\n",
    "    'stage4': {\n",
    "        'pooling_method': ['hidden state', 'max pooling', 'mean pooling'],\n",
    "        'activation_function': [None, 'ReLU', 'tanh', 'softmax']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd78d70",
   "metadata": {},
   "source": [
    "## 步驟3：資料清理函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cfc3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步驟3：資料清理函數\n",
    "def clean_text(text):\n",
    "    # 轉換為小寫\n",
    "    text = text.lower()\n",
    "    # 移除網址\n",
    "    text = re.sub(r'http\\\\S+|www\\\\S+|https\\\\S+', '', text, flags=re.MULTILINE)\n",
    "    # 移除標點符號\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # 移除非英文字母\n",
    "    text = re.sub(r'[^a-z\\\\s]', '', text)\n",
    "    # 自定義停用詞（保留情感相關詞彙）\n",
    "    custom_stop_words = set(stopwords.words('english')) - {'not', 'very', 'really'}\n",
    "    text = ' '.join(word for word in text.split() if word not in custom_stop_words)\n",
    "    # 移除多餘空格\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424aa051",
   "metadata": {},
   "source": [
    "## 步驟4：讀取資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a103c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Corona_NLP_train.csv', encoding='latin1')\n",
    "test_df = pd.read_csv('Corona_NLP_test.csv', encoding='latin1')\n",
    "\n",
    "# 清理資料\n",
    "train_df['clean_text'] = train_df['OriginalTweet'].apply(clean_text)\n",
    "test_df['clean_text'] = test_df['OriginalTweet'].apply(clean_text)\n",
    "\n",
    "# 檢查清理後文本長度\n",
    "text_lengths = train_df['clean_text'].apply(lambda x: len(x.split()))\n",
    "print(f\"平均長度: {text_lengths.mean():.2f}, 最大長度: {text_lengths.max()}\")\n",
    "\n",
    "print(\"清理後的訓練資料前5筆：\")\n",
    "print(train_df[['OriginalTweet', 'clean_text']].head())\n",
    "print(\"\\n清理後的測試資料前5筆：\")\n",
    "print(test_df[['OriginalTweet', 'clean_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a06ff29",
   "metadata": {},
   "source": [
    "## 步驟5：標籤編碼 & class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84056b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_mapping = {\n",
    "    'Extremely Negative': 0,\n",
    "    'Negative': 1,\n",
    "    'Neutral': 2,\n",
    "    'Positive': 3,\n",
    "    'Extremely Positive': 4\n",
    "}\n",
    "train_df['label'] = train_df['Sentiment'].map(sentiment_mapping)\n",
    "test_df['label'] = test_df['Sentiment'].map(sentiment_mapping)\n",
    "\n",
    "print(\"\\n訓練資料類別分佈：\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(\"\\n測試資料類別分佈：\")\n",
    "print(test_df['label'].value_counts())\n",
    "\n",
    "# 計算類別權重\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label'])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f1f1de",
   "metadata": {},
   "source": [
    "## 步驟 6：創建自定義資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a365d530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70110f14",
   "metadata": {},
   "source": [
    "## 步驟7：定義 BERT-BiLSTM 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ce86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBiLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers, dropout, pooling_method, activation_function):\n",
    "        super(BertBiLSTM, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        for param in self.bert.encoder.layer[:10].parameters():\n",
    "            param.requires_grad = False\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=768,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pooling_method = pooling_method\n",
    "        if pooling_method == 'hidden state':\n",
    "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim * 4, output_dim)\n",
    "        self.activation_function = activation_function\n",
    "        if activation_function == 'ReLU':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation_function == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation_function == 'softmax':\n",
    "            self.activation = nn.Softmax(dim=1)\n",
    "        else:\n",
    "            self.activation = None\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embedded = bert_outputs[0]\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        if self.pooling_method == 'hidden state':\n",
    "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "            pooled = hidden\n",
    "        elif self.pooling_method == 'max pooling':\n",
    "            max_pool, _ = torch.max(lstm_out, dim=1)\n",
    "            avg_pool = torch.mean(lstm_out, dim=1)\n",
    "            pooled = torch.cat((max_pool, avg_pool), dim=1)\n",
    "        elif self.pooling_method == 'mean pooling':\n",
    "            pooled = torch.mean(lstm_out, dim=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid pooling_method: {self.pooling_method}. Expected 'hidden state', 'max pooling', or 'mean pooling'.\")\n",
    "        pooled = self.dropout(pooled)\n",
    "        output = self.fc(pooled)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145d910",
   "metadata": {},
   "source": [
    "## 步驟8：訓練與驗證函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934829bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in tqdm(data_loader, desc=\"訓練\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"驗證\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    return total_loss / len(data_loader), correct / total, f1, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b668e",
   "metadata": {},
   "source": [
    "## 步驟 9：階段性參數實驗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f7130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def staged_search():\n",
    "    best_params = default_params.copy()\n",
    "    best_val_acc = 0\n",
    "    results = []\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # 階段一：batch_size 和 learning_rate\n",
    "    print(\"\\n階段一：測試 batch_size 和 learning_rate\")\n",
    "    for batch_size, learning_rate in product(stage_params['stage1']['batch_size'], stage_params['stage1']['learning_rate']):\n",
    "        print(f\"\\n測試參數：batch_size={batch_size}, learning_rate={learning_rate}\")\n",
    "        train_dataset = TweetDataset(\n",
    "            texts=train_df['clean_text'].to_numpy(),\n",
    "            labels=train_df['label'].to_numpy(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=best_params['max_length']\n",
    "        )\n",
    "        test_dataset = TweetDataset(\n",
    "            texts=test_df['clean_text'].to_numpy(),\n",
    "            labels=test_df['label'].to_numpy(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=best_params['max_length']\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "        model = BertBiLSTM(\n",
    "            hidden_dim=best_params['hidden_dim'],\n",
    "            output_dim=OUTPUT_DIM,\n",
    "            num_layers=best_params['num_layers'],\n",
    "            dropout=best_params['dropout'],\n",
    "            pooling_method=best_params['pooling_method'],\n",
    "            activation_function=best_params['activation_function']\n",
    "        ).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "        optimizer = optim.Adam([\n",
    "            {'params': model.bert.parameters(), 'lr': learning_rate, 'weight_decay': 1e-4},\n",
    "            {'params': list(model.lstm.parameters()) + list(model.fc.parameters()), 'lr': learning_rate, 'weight_decay': 1e-4}\n",
    "        ])\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "        best_val_loss = float('inf')\n",
    "        counter = 0\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            val_loss, val_acc, val_f1, _, _ = evaluate(model, test_loader, criterion, device)\n",
    "            scheduler.step()\n",
    "            print(f'輪次 {epoch+1}/{EPOCHS}, 訓練損失: {train_loss:.4f}, 訓練準確率: {train_acc:.4f}')\n",
    "            print(f'驗證損失: {val_loss:.4f}, 驗證準確率: {val_acc:.4f}, F1: {val_f1:.4f}')\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= PATIENCE:\n",
    "                    print(f'早停於輪次 {epoch+1}')\n",
    "                    break\n",
    "        results.append({\n",
    "            'stage': 1,\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'val_acc': val_acc,\n",
    "            'val_f1': val_f1\n",
    "        })\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_params['batch_size'] = batch_size\n",
    "            best_params['learning_rate'] = learning_rate\n",
    "    print(f\"\\n階段一最佳參數：batch_size={best_params['batch_size']}, learning_rate={best_params['learning_rate']}, 驗證準確率={best_val_acc:.4f}\")\n",
    "\n",
    "    # 階段二：hidden_dim, num_layers, dropout\n",
    "    print(\"\\n階段二：測試 hidden_dim, num_layers, dropout\")\n",
    "    for hidden_dim, num_layers, dropout in product(stage_params['stage2']['hidden_dim'], stage_params['stage2']['num_layers'], stage_params['stage2']['dropout']):\n",
    "        print(f\"\\n測試參數：hidden_dim={hidden_dim}, num_layers={num_layers}, dropout={dropout}\")\n",
    "        train_dataset = TweetDataset(\n",
    "            texts=train_df['clean_text'].to_numpy(),\n",
    "            labels=train_df['label'].to_numpy(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=best_params['max_length']\n",
    "        )\n",
    "        test_dataset = TweetDataset(\n",
    "            texts=test_df['clean_text'].to_numpy(),\n",
    "            labels=test_df['label'].to_numpy(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=best_params['max_length']\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'])\n",
    "        model = BertBiLSTM(\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=OUTPUT_DIM,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            pooling_method=best_params['pooling_method'],\n",
    "            activation_function=best_params['activation_function']\n",
    "        ).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "        optimizer = optim.Adam([\n",
    "            {'params': model.bert.parameters(), 'lr': best_params['learning_rate'], 'weight_decay': 1e-4},\n",
    "            {'params': list(model.lstm.parameters()) + list(model.fc.parameters()), 'lr': best_params['learning_rate'], 'weight_decay': 1e-4}\n",
    "        ])\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "        best_val_loss = float('inf')\n",
    "        counter = 0\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            val_loss, val_acc, val_f1, _, _ = evaluate(model, test_loader, criterion, device)\n",
    "            scheduler.step()\n",
    "            print(f'輪次 {epoch+1}/{EPOCHS}, 訓練損失: {train_loss:.4f}, 訓練準確率: {train_acc:.4f}')\n",
    "            print(f'驗證損失: {val_loss:.4f}, 驗證準確率: {val_acc:.4f}, F1: {val_f1:.4f}')\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= PATIENCE:\n",
    "                    print(f'早停於輪次 {epoch+1}')\n",
    "                    break\n",
    "        results.append({\n",
    "            'stage': 2,\n",
    "            'hidden_dim': hidden_dim,\n",
    "            'num_layers': num_layers,\n",
    "            'dropout': dropout,\n",
    "            'val_acc': val_acc,\n",
    "            'val_f1': val_f1\n",
    "        })\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_params['hidden_dim'] = hidden_dim\n",
    "            best_params['num_layers'] = num_layers\n",
    "            best_params['dropout'] = dropout\n",
    "    print(f\"\\n階段二最佳參數：hidden_dim={best_params['hidden_dim']}, num_layers={best_params['num_layers']}, dropout={best_params['dropout']}, 驗證準確率={best_val_acc:.4f}\")\n",
    "\n",
    "    # 階段三：max_length\n",
    "    print(\"\\n階段三：測試 max_length\")\n",
    "    for max_length in stage_params['stage3']['max_length']:\n",
    "        print(f\"\\n測試參數：max_length={max_length}\")\n",
    "        train_dataset = TweetDataset(\n",
    "            texts=train_df['clean_text'].to_numpy(),\n",
    "            labels=train_df['label'].to_numpy(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=max_length\n",
    "        )\n",
    "        test_dataset = TweetDataset(\n",
    "            texts=test_df['clean_text'].to_numpy(),\n",
    "            labels=test_df['label'].to_numpy(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=max_length\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'])\n",
    "        model = BertBiLSTM(\n",
    "            hidden_dim=best_params['hidden_dim'],\n",
    "            output_dim=OUTPUT_DIM,\n",
    "            num_layers=best_params['num_layers'],\n",
    "            dropout=best_params['dropout'],\n",
    "            pooling_method=best_params['pooling_method'],\n",
    "            activation_function=best_params['activation_function']\n",
    "        ).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "        optimizer = optim.Adam([\n",
    "            {'params': model.bert.parameters(), 'lr': best_params['learning_rate'], 'weight_decay': 1e-4},\n",
    "            {'params': list(model.lstm.parameters()) + list(model.fc.parameters()), 'lr': best_params['learning_rate'], 'weight_decay': 1e-4}\n",
    "        ])\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "        best_val_loss = float('inf')\n",
    "        counter = 0\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            val_loss, val_acc, val_f1, _, _ = evaluate(model, test_loader, criterion, device)\n",
    "            scheduler.step()\n",
    "            print(f'輪次 {epoch+1}/{EPOCHS}, 訓練損失: {train_loss:.4f}, 訓練準確率: {train_acc:.4f}')\n",
    "            print(f'驗證損失: {val_loss:.4f}, 驗證準確率: {val_acc:.4f}, F1: {val_f1:.4f}')\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= PATIENCE:\n",
    "                    print(f'早停於輪次 {epoch+1}')\n",
    "                    break\n",
    "        results.append({\n",
    "            'stage': 3,\n",
    "            'max_length': max_length,\n",
    "            'val_acc': val_acc,\n",
    "            'val_f1': val_f1\n",
    "        })\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_params['max_length'] = max_length\n",
    "    print(f\"\\n階段三最佳參數：max_length={best_params['max_length']}, 驗證準確率={best_val_acc:.4f}\")\n",
    "\n",
    "    # 階段四：pooling_method 和 activation_function\n",
    "    print(\"\\n階段四：測試 pooling_method 和 activation_function\")\n",
    "    for pooling_method, activation_function in product(stage_params['stage4']['pooling_method'], stage_params['stage4']['activation_function']):\n",
    "        print(f\"\\n測試參數：pooling_method={pooling_method}, activation_function={activation_function}\")\n",
    "        train_dataset = TweetDataset(\n",
    "            texts=train_df['clean_text'].to_numpy(),\n",
    "            labels=train_df['label'].to_numpy(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=best_params['max_length']\n",
    "        )\n",
    "        test_dataset = TweetDataset(\n",
    "            texts=test_df['clean_text'].to_numpy(),\n",
    "            labels=test_df['label'].to_numpy(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=best_params['max_length']\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'])\n",
    "        model = BertBiLSTM(\n",
    "            hidden_dim=best_params['hidden_dim'],\n",
    "            output_dim=OUTPUT_DIM,\n",
    "            num_layers=best_params['num_layers'],\n",
    "            dropout=best_params['dropout'],\n",
    "            pooling_method=pooling_method,\n",
    "            activation_function=activation_function\n",
    "        ).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "        optimizer = optim.Adam([\n",
    "            {'params': model.bert.parameters(), 'lr': best_params['learning_rate'], 'weight_decay': 1e-4},\n",
    "            {'params': list(model.lstm.parameters()) + list(model.fc.parameters()), 'lr': best_params['learning_rate'], 'weight_decay': 1e-4}\n",
    "        ])\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "        best_val_loss = float('inf')\n",
    "        counter = 0\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            val_loss, val_acc, val_f1, _, _ = evaluate(model, test_loader, criterion, device)\n",
    "            scheduler.step()\n",
    "            print(f'輪次 {epoch+1}/{EPOCHS}, 訓練損失: {train_loss:.4f}, 訓練準確率: {train_acc:.4f}')\n",
    "            print(f'驗證損失: {val_loss:.4f}, 驗證準確率: {val_acc:.4f}, F1: {val_f1:.4f}')\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= PATIENCE:\n",
    "                    print(f'早停於輪次 {epoch+1}')\n",
    "                    break\n",
    "        results.append({\n",
    "            'stage': 4,\n",
    "            'pooling_method': pooling_method,\n",
    "            'activation_function': activation_function,\n",
    "            'val_acc': val_acc,\n",
    "            'val_f1': val_f1\n",
    "        })\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_params['pooling_method'] = pooling_method\n",
    "            best_params['activation_function'] = activation_function\n",
    "    print(f\"\\n階段四最佳參數：pooling_method={best_params['pooling_method']}, activation_function={best_params['activation_function']}, 驗證準確率={best_val_acc:.4f}\")\n",
    "\n",
    "    # 儲存結果\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('staged_search_results.csv', index=False)\n",
    "    print(f\"\\n最終最佳參數：{best_params}\")\n",
    "    print(f\"最終最佳驗證準確率: {best_val_acc:.4f}\")\n",
    "\n",
    "    # 使用最佳參數進行最終訓練並生成混淆矩陣\n",
    "    print(\"\\n使用最佳參數進行最終訓練...\")\n",
    "    train_dataset = TweetDataset(\n",
    "        texts=train_df['clean_text'].to_numpy(),\n",
    "        labels=train_df['label'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=best_params['max_length']\n",
    "    )\n",
    "    test_dataset = TweetDataset(\n",
    "        texts=test_df['clean_text'].to_numpy(),\n",
    "        labels=test_df['label'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=best_params['max_length']\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'])\n",
    "    model = BertBiLSTM(\n",
    "        hidden_dim=best_params['hidden_dim'],\n",
    "        output_dim=OUTPUT_DIM,\n",
    "        num_layers=best_params['num_layers'],\n",
    "        dropout=best_params['dropout'],\n",
    "        pooling_method=best_params['pooling_method'],\n",
    "        activation_function=best_params['activation_function']\n",
    "    ).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model.bert.parameters(), 'lr': best_params['learning_rate'], 'weight_decay': 1e-4},\n",
    "        {'params': list(model.lstm.parameters()) + list(model.fc.parameters()), 'lr': best_params['learning_rate'], 'weight_decay': 1e-4}\n",
    "    ])\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, val_f1, all_preds, all_labels = evaluate(model, test_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        print(f'輪次 {epoch+1}/{EPOCHS}')\n",
    "        print(f'訓練損失: {train_loss:.4f}, 訓練準確率: {train_acc:.4f}')\n",
    "        print(f'驗證損失: {val_loss:.4f}, 驗證準確率: {val_acc:.4f}, F1: {val_f1:.4f}')\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= PATIENCE:\n",
    "                print(f'早停於輪次 {epoch+1}')\n",
    "                break\n",
    "\n",
    "    # 載入最佳模型\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    # 生成混淆矩陣\n",
    "    sentiment_labels = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=sentiment_labels, yticklabels=sentiment_labels)\n",
    "    plt.xlabel('預測標籤')\n",
    "    plt.ylabel('實際標籤')\n",
    "    plt.title('混淆矩陣 (BERT + BiLSTM)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix_bert_bilstm.png')\n",
    "    plt.show()\n",
    "\n",
    "    # 繪製訓練/驗證曲線\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(train_losses)+1), train_losses, label='訓練損失')\n",
    "    plt.plot(range(1, len(val_losses)+1), val_losses, label='驗證損失')\n",
    "    plt.xlabel('輪次')\n",
    "    plt.ylabel('損失')\n",
    "    plt.title('訓練/驗證損失曲線')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(val_accuracies)+1), val_accuracies, label='驗證準確率', color='green')\n",
    "    plt.xlabel('輪次')\n",
    "    plt.ylabel('準確率')\n",
    "    plt.title('驗證準確率曲線')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('final_training_curves.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009d2a26",
   "metadata": {},
   "source": [
    "## 步驟10：最終訓練與視覺化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef6644",
   "metadata": {},
   "outputs": [],
   "source": [
    "staged_search()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
